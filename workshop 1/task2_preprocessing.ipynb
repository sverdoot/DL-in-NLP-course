{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sverdoot/DL-in-NLP-course/blob/master/workshop%201/task2_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "55l9V_gkHdLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2vec preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "F_L94-g_HdLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preprocessing is not the most interesting part of NLP, but it is still one of the most important ones. Your task is to preprocess raw text (you can use your own, or [this one](http://mattmahoney.net/dc/textdata). For this task text preprocessing mostly consists of:\n",
        "\n",
        "1. cleaning (mostly, if your dataset is from social media or parced from the internet)\n",
        "1. tokenization\n",
        "1. building the vocabulary and choosing its size\n",
        "1. assigning each token a number (numericalization)\n",
        "1. data structuring and batching\n",
        "\n",
        "Your goal is to make SkipGramBatcher class which returns two numpy tensors with word indices. You can implement batcher for Skip-Gram or CBOW architecture, the picture below can be helpfull to remember the difference.\n",
        "\n",
        "![text](https://raw.githubusercontent.com/deepmipt/deep-nlp-seminars/651804899d05b96fc72b9474404fab330365ca09/seminar_02/pics/architecture.png)\n",
        "\n",
        "There are several ways to do it right. Shapes could be `(batch_size, 2*window_size)`, `(batch_size,)` for CBOW or `(batch_size,)`, `(batch_size,)` for Skip-Gram. You should **not** do negative sampling here.\n",
        "\n",
        "They should be adequately parametrized: CBOW(batch_size, window_size, ...), SkipGram(num_skips, skip_window). You should implement only one batcher in this task, it's up to you which one to chose.\n",
        "\n",
        "Useful links:\n",
        "1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
        "\n",
        "You can write the code in this notebook, or in separate file. It will be reused for the next task. Result of your work should represent that your batch have proper structure (right shapes) and content (words should be from one context, not some random indices). To show that, translate indices back to words and print them to show something like this:\n",
        "\n",
        "```\n",
        "bag_window = 2\n",
        "\n",
        "batch = [['first', 'used', 'early', 'working'],\n",
        "        ['used', 'against', 'working', 'class'],\n",
        "        ['against', 'early', 'class', 'radicals'],\n",
        "        ['early', 'working', 'radicals', 'including']]\n",
        "\n",
        "labels = ['against', 'early', 'working', 'class']\n",
        "```\n",
        "\n",
        "If you struggle with somethng, ask your neighbour. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - just do it! Good luck!"
      ]
    },
    {
      "metadata": {
        "id": "TWiZGx-Z2V7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(42)\n",
        "import numpy as np\n",
        "\n",
        "import collections, math, os, random, zipfile\n",
        "import numpy as np\n",
        "\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange\n",
        "\n",
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "import requests, os\n",
        "from os.path import isfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vE9RgLRK2aS7",
        "colab_type": "code",
        "outputId": "5f517242-cf2b-4a7f-d3a9-8ea494f74713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('text8'):\n",
        "    with open('data.zip', 'wb') as f:\n",
        "        r = requests.get('http://mattmahoney.net/dc/text8.zip')\n",
        "        f.write(r.content)\n",
        "    !unzip 'data.zip' \n",
        "\n",
        "def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = (f.read(f.namelist()[0]).split())\n",
        "        data = list(map(lambda x: x.decode(), data))\n",
        "    return data\n",
        "\n",
        "!ls\n",
        "#words = read_data('text8')\n",
        "with open('text8') as f:\n",
        "    words = f.read().split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: text8                   \n",
            "data.zip  sample_data  text8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2-bQfj0mqmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "full_data = words\n",
        "freq_dict = collections.Counter(full_data)\n",
        "no_rare_dict = freq_dict.most_common(VOCAB_SIZE)\n",
        "min_freq = no_rare_dict[-1][1]\n",
        "vocabulary = [x[0] for x in no_rare_dict]\n",
        "vocabulary.append(UNK_TOKEN)\n",
        "\n",
        "data = []\n",
        "for i, word in enumerate(full_data):\n",
        "  if freq_dict[word] > min_freq:\n",
        "    data.append(word)\n",
        "  else:\n",
        "    data.append(UNK_TOKEN)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "data_idx = 0\n",
        "\n",
        "class Batcher:\n",
        "  \n",
        "  def __init__(self, batch_size, window_size, data, data_idx=0):\n",
        "    self.batch_size = batch_size\n",
        "    self.window_size = window_size\n",
        "    self.data = data\n",
        "    \n",
        "  def __iter__(self):\n",
        "  \n",
        "    return self\n",
        "\n",
        "  def __next__(self, data_idx_=None):\n",
        "    window_size = self.window_size\n",
        "    batch_size = self.batch_size\n",
        "    data = self.data\n",
        "    global data_idx\n",
        "    batch = []\n",
        "    labels = [] \n",
        "  \n",
        "    context = collections.deque(maxlen=2 * window_size + 1)\n",
        "  \n",
        "    for _ in range(2 * window_size + 1):\n",
        "      context.append(word2idx[data[data_idx]])\n",
        "      data_idx = (data_idx + 1) % len(data)\n",
        "  \n",
        "    for i in range(batch_size):\n",
        "      batch.append([context[j] for  j in list(range(window_size)) + list(range(window_size + 1, 2 * window_size + 1))])\n",
        "      labels.append(context[window_size])\n",
        "      context.append(word2idx[data[data_idx]])\n",
        "      data_idx = (data_idx + 1) % len(data)\n",
        "    \n",
        "    return (batch, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbMARmff2vMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "window_size = 2\n",
        "batcher = Batcher(batch_size, window_size, data)\n",
        "build_batch = iter(batcher)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q4LpzZQMlAdg",
        "colab_type": "code",
        "outputId": "eafc2ad8-b68c-48ce-d6c9-5329d5be6f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "print('data:', [di for di in data[:16]])\n",
        "\n",
        "data_index = 0\n",
        "batch, labels = next(build_batch)\n",
        "print('\\nwith window_size = %d:' %2)\n",
        "print('    batch:', [[idx2word[b] for b in bi] for bi in batch])\n",
        "print('    labels:', [idx2word[li] for li in labels])\n",
        "data: ['', 'a', 'the', 'cat', 'and', 'a', 'the', 'the']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', '<UNK>', 'including', 'the']\n",
            "\n",
            "with window_size = 2:\n",
            "    batch: [['anarchism', 'originated', 'a', 'term'], ['originated', 'as', 'term', 'of'], ['as', 'a', 'of', 'abuse'], ['a', 'term', 'abuse', 'first'], ['term', 'of', 'first', 'used'], ['of', 'abuse', 'used', 'against'], ['abuse', 'first', 'against', 'early'], ['first', 'used', 'early', 'working'], ['used', 'against', 'working', 'class'], ['against', 'early', 'class', '<UNK>'], ['early', 'working', '<UNK>', 'including'], ['working', 'class', 'including', 'the'], ['class', '<UNK>', 'the', '<UNK>'], ['<UNK>', 'including', '<UNK>', 'of'], ['including', 'the', 'of', 'the'], ['the', '<UNK>', 'the', 'english']]\n",
            "    labels: ['as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', '<UNK>', 'including', 'the', '<UNK>', 'of']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}